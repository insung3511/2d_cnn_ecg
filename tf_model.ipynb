{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래픽 카드 둘로 쓰기 (두개 있을때, 하나만 있다면 0)\n",
    "# gpu idx 를 0 또는 1 로 설정하시오\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"{}\".format(1) # gpu idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 539628728417002866\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로에 폴더가 없으면 폴더 만들기\n",
    "import os\n",
    "\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pathlib\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "EPOCH = 100\n",
    "KERNEL_SIZE = 3\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "\n",
    "DATA_PATH = \"./graph_data/\"\n",
    "\n",
    "def list_to_list(input_list):\n",
    "    input_list_to_list = list(itertools.chain(*input_list))\n",
    "    return input_list_to_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_data\n",
      "112599\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path(DATA_PATH)\n",
    "print(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "print(image_count)\n",
    "\n",
    "f = list(data_dir.glob('F/*'))\n",
    "n = list(data_dir.glob('N/*'))\n",
    "q = list(data_dir.glob('Q/*'))\n",
    "s = list(data_dir.glob('S/*'))\n",
    "v = list(data_dir.glob('V/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 split\n",
    "## train, test, validation data 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_path = DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current path : ./graph_data/F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 803/803 [00:05<00:00, 137.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current path : ./graph_data/N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90631/90631 [11:12<00:00, 134.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current path : ./graph_data/Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11148/11148 [01:19<00:00, 139.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current path : ./graph_data/S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2781/2781 [00:19<00:00, 141.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current path : ./graph_data/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7236/7236 [00:53<00:00, 135.63it/s]\n"
     ]
    }
   ],
   "source": [
    "parents_path = DATA_PATH\n",
    "child_path = os.listdir(parents_path)\n",
    "\n",
    "npy_check_list = []\n",
    "\n",
    "temp_converted_img = list()\n",
    "temp_ann_list = list()\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "for pic_path in (child_path):\n",
    "    current_path = os.listdir(parents_path + pic_path)\n",
    "    print(\"[INFO] Current path : \" + parents_path + pic_path)\n",
    "    for file_name in tqdm(current_path):\n",
    "        path_for_array = parents_path + pic_path + \"/\" + file_name\n",
    "\n",
    "        img = cv2.imread(path_for_array)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img_resize = cv2.resize(img, dsize=(128, 128), interpolation=cv2.INTER_AREA)\n",
    "        temp_converted_img.append(img_resize / 255.0)\n",
    "        \n",
    "        check_ann = pic_path\n",
    "        \n",
    "        if check_ann == \"N\":            # Normal\n",
    "            temp_ann_list.append(0)\n",
    "        \n",
    "        elif check_ann == \"S\":          # Supra-ventricular\n",
    "            temp_ann_list.append(1)\n",
    "        \n",
    "        elif check_ann == \"V\":          # Ventricular\n",
    "            temp_ann_list.append(2)\n",
    "        \n",
    "        elif check_ann == \"F\":          # False alarm\n",
    "            temp_ann_list.append(3)\n",
    "        \n",
    "        else:                           # Unclassed \n",
    "            temp_ann_list.append(4)\n",
    "    \n",
    "        y.append(temp_ann_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_y = []\n",
    "for y_class in y:\n",
    "    for y_value in y_class:\n",
    "        if y_value == 0:#    N, S, V, F, Q\n",
    "            onehot_y.append([1, 0, 0, 0, 0])\n",
    "        \n",
    "        elif y_value == 1:#  N, S, V, F, Q\n",
    "            onehot_y.append([0, 1, 0, 0, 0])\n",
    "        \n",
    "        elif y_value == 2:#  N, S, V, F, Q\n",
    "            onehot_y.append([0, 0, 1, 0, 0])\n",
    "        \n",
    "        elif y_value == 3:#  N, S, V, F, Q\n",
    "            onehot_y.append([0, 0, 0, 1, 0])\n",
    "        \n",
    "        else:#               N, S, V, F, Q\n",
    "            onehot_y.append([0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_y = np.array(onehot_y)\n",
    "temp_converted_img = np.array(temp_converted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(temp_converted_img, onehot_y, test_size=0.33, random_state=42, shuffle=True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"[SIZE]\\t\\tNpX lenght : {}\\n\\t\\tNpY length : {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"[SIZE]\\t\\tX_validation length : {}\\n\\t\\ty_validation length : {}\".format(X_val.shape, y_val.shape))\n",
    "print(\"[SIZE]\\t\\tX_test length : {}\\n\\t\\ty_test length : {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤으로 뽑아서 뿌려보기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(30, 12))\n",
    "plt.suptitle(\"random pal, per PAL\", fontsize=18)\n",
    "n = 0\n",
    "random.seed(11)\n",
    "for i in random.sample(range(6480), 16):\n",
    "    ax = plt.subplot(5, 5, n+1)\n",
    "    plt.imshow(temp_converted_img[i] * 255.0, interpolation='nearest')\n",
    "    ax.set_title(str(onehot_y[i]))\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (128, 128, 1)\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    models = keras.Sequential([\n",
    "        # tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_size),\n",
    "        layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "        layers.Conv2D(128, kernel_size=(2, 2), activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "        layers.Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "        layers.Conv2D(512, kernel_size=(2, 2), activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dense(5, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.compile(\n",
    "        optimizer='adam',\n",
    "        # loss=\"sparse_categorical_crossentropy\",\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 콜백 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 설정\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "outDir = './cheakpoint/lefms_model/' # 이 경로에 best 모델이 저장된다.\n",
    "model_names = outDir + 'weights-{val_accuracy:.4f}.h5'\n",
    "\n",
    "def get_callbacks(patience = 50):\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=patience)\n",
    "    model_checkpoint = ModelCheckpoint(model_names, monitor='val_accuracy', verbose=1, save_best_only=True, period = 1)\n",
    "  \n",
    "    # callbacks = [earlystop, model_checkpoint]     # earlystop 사용하고 싶으면 이거 풀고 아래꺼 주석 처리\n",
    "    callbacks = [model_checkpoint]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks()\n",
    "\n",
    "models_hist = models.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCH,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks = [callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 시각화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 된 모델의 학습 과정 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_model__hist(hist):\n",
    "    path = './cheakpoint/lefms/' # loss, accuracy 그래프 저장할 path\n",
    "    createDirectory(path)\n",
    "\n",
    "    # loss 추이 그래프로 그려서 저장\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.plot(hist.history['loss'], color='b', label=\"Training loss\")\n",
    "    plt.plot(hist.history['val_loss'], color='r', label=\"Validation loss\")\n",
    "    plt.savefig(path + 'model_loss_hist.png')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # accuracy 추이 그래프로 그려서 저장\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "    plt.plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "    plt.savefig(path + 'model_loss_hist.png')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model__hist(models_hist)\n",
    "loss,acc = models.evaluate((X_val, y_val), verbose=2)\n",
    "print(\"multi_model의 정확도: {:5.2f}%\".format(100*acc))\n",
    "print(\"multi_model의 Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 불러와서 confusion matrix 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "reconstructed_model = keras.models.load_model(\"./cheakpoint/lefms_model/weights-1.0000.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 얻기\n",
    "y_pred = reconstructed_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hat encoding 를 하나의 변수로 바꾸기\n",
    "new_y= []\n",
    "for val in y_test:\n",
    "    max = 0\n",
    "    cnt = 0\n",
    "    for idx, num in enumerate(val):\n",
    "        if max < num:\n",
    "            max = num\n",
    "            cnt = idx + 1\n",
    "    new_y.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hat encoding 를 하나의 변수로 바꾸기\n",
    "new_y_pred = []\n",
    "for val in y_pred:\n",
    "    max = 0\n",
    "    cnt = 0\n",
    "    for idx, num in enumerate(val):\n",
    "        if max < num:\n",
    "            max = num\n",
    "            cnt = idx + 1\n",
    "    new_y_pred.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 정확도 산출\n",
    "score = reconstructed_model.evaluate(X_test, y_test, verbose=1)\n",
    "print('정답률 = ', score[1],'loss=', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개수 버전\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "cm2 = confusion_matrix(new_y, new_y_pred)\n",
    "sns.heatmap(cm2, annot = True, fmt = 'd', cmap= 'Reds')\n",
    "plt.xlabel('predict')\n",
    "plt.ylabel('real')\n",
    "plt.xticks([0.5, 1.5, 2.5, 3.5, 4.5], ['0 = N', '1 = S', '2 = V', '3 = F', '4 = Q'])\n",
    "plt.yticks([0.5, 1.5, 2.5, 3.5, 4.5], ['0 = N', '1 = S', '2 = V', '3 = F', '4 = Q'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile 버전\n",
    "total = np.sum(cm2, axis=1)\n",
    "cm2_percentile = cm2/total[:,None]\n",
    "sns.heatmap(np.round(cm2_percentile,3), annot = True, cmap= 'Reds')\n",
    "plt.xlabel('predict')\n",
    "plt.ylabel('real')\n",
    "plt.xticks([0.5, 1.5, 2.5, 3.5, 4.5], ['0 = N', '1 = S', '2 = V', '3 = F', '4 = Q'])\n",
    "plt.yticks([0.5, 1.5, 2.5, 3.5, 4.5], ['0 = N', '1 = S', '2 = V', '3 = F', '4 = Q'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(new_y, new_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_report 그리기\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0 = N', '1 = S', '2 = V', '3 = F', '4 = Q']\n",
    "print(classification_report(new_y, new_y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
